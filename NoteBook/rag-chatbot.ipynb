{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "\n",
    "<https://learn.microsoft.com/en-us/azure/cosmos-db/gen-ai/rag-chatbot>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --user python-dotenv\n",
    "! pip install --user aiohttp\n",
    "! pip install --user openai\n",
    "! pip install --user gradio\n",
    "! pip install --user ijson\n",
    "! pip install --user nest_asyncio\n",
    "! pip install --user tenacity\n",
    "# Note: ensure you have azure-cosmos version 4.7 or higher installed\n",
    "! pip install --user azure-cosmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cosmos_uri = \"https://<replace with cosmos db account name>.documents.azure.com:443/\"\n",
    "cosmos_key = \"<replace with cosmos db account key>\"\n",
    "cosmos_database_name = \"database\"\n",
    "cosmos_collection_name = \"vectorstore\"\n",
    "cosmos_vector_property_name = \"vector\"\n",
    "cosmos_cache_database_name = \"database\"\n",
    "cosmos_cache_collection_name = \"vectorcache\"\n",
    "openai_endpoint = \"<replace with azure openai endpoint>\"\n",
    "openai_key = \"<replace with azure openai key>\"\n",
    "openai_type = \"azure\"\n",
    "openai_api_version = \"2023-05-15\"\n",
    "openai_embeddings_deployment = \"<replace with azure openai embeddings deployment name>\"\n",
    "openai_embeddings_model = \"<replace with azure openai embeddings model - e.g. text-embedding-3-large\"\n",
    "openai_embeddings_dimensions = \"1536\"\n",
    "openai_completions_deployment = \"<replace with azure openai completions deployment name>\"\n",
    "openai_completions_model = \"<replace with azure openai completions model - e.g. gpt-35-turbo>\"\n",
    "storage_file_url = \"https://cosmosdbcosmicworks.blob.core.windows.net/fabcondata/movielens_dataset.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import time\n",
    "import json\n",
    "import uuid\n",
    "import urllib \n",
    "import ijson\n",
    "import zipfile\n",
    "from dotenv import dotenv_values\n",
    "from openai import AzureOpenAI\n",
    "from azure.core.exceptions import AzureError\n",
    "from azure.cosmos import PartitionKey, exceptions\n",
    "from time import sleep\n",
    "import gradio as gr\n",
    "\n",
    "# Cosmos DB imports\n",
    "from azure.cosmos import CosmosClient\n",
    "\n",
    "# Load configuration\n",
    "env_name = \"sample_env_file.env\"\n",
    "config = dotenv_values(env_name)\n",
    "\n",
    "cosmos_conn = config['cosmos_uri']\n",
    "cosmos_key = config['cosmos_key']\n",
    "cosmos_database = config['cosmos_database_name']\n",
    "cosmos_collection = config['cosmos_collection_name']\n",
    "cosmos_vector_property = config['cosmos_vector_property_name']\n",
    "comsos_cache_db = config['cosmos_cache_database_name']\n",
    "cosmos_cache = config['cosmos_cache_collection_name']\n",
    "\n",
    "# Create the Azure Cosmos DB for NoSQL async client for faster data loading\n",
    "cosmos_client = CosmosClient(url=cosmos_conn, credential=cosmos_key)\n",
    "\n",
    "openai_endpoint = config['openai_endpoint']\n",
    "openai_key = config['openai_key']\n",
    "openai_api_version = config['openai_api_version']\n",
    "openai_embeddings_deployment = config['openai_embeddings_deployment']\n",
    "openai_embeddings_dimensions = int(config['openai_embeddings_dimensions'])\n",
    "openai_completions_deployment = config['openai_completions_deployment']\n",
    "\n",
    "# Movies file url\n",
    "storage_file_url = config['storage_file_url']\n",
    "\n",
    "# Create the OpenAI client\n",
    "openai_client = AzureOpenAI(azure_endpoint=openai_endpoint, api_key=openai_key, api_version=openai_api_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = cosmos_client.create_database_if_not_exists(cosmos_database)\n",
    "\n",
    "# Create the vector embedding policy to specify vector details\n",
    "vector_embedding_policy = {\n",
    "    \"vectorEmbeddings\": [ \n",
    "        { \n",
    "            \"path\":\"/\" + cosmos_vector_property,\n",
    "            \"dataType\":\"float32\",\n",
    "            \"distanceFunction\":\"cosine\",\n",
    "            \"dimensions\":openai_embeddings_dimensions\n",
    "        }, \n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the vector index policy to specify vector details\n",
    "indexing_policy = {\n",
    "    \"includedPaths\": [ \n",
    "    { \n",
    "        \"path\": \"/*\" \n",
    "    } \n",
    "    ], \n",
    "    \"excludedPaths\": [ \n",
    "    { \n",
    "        \"path\": \"/\\\"_etag\\\"/?\",\n",
    "        \"path\": \"/\" + cosmos_vector_property + \"/*\",\n",
    "    } \n",
    "    ], \n",
    "    \"vectorIndexes\": [ \n",
    "        {\n",
    "            \"path\": \"/\"+cosmos_vector_property, \n",
    "            \"type\": \"quantizedFlat\" \n",
    "        }\n",
    "    ]\n",
    "} \n",
    "\n",
    "# Create the data collection with vector index (note: this creates a container with 10000 RUs to allow fast data load)\n",
    "try:\n",
    "    movies_container = db.create_container_if_not_exists(id=cosmos_collection, \n",
    "                                                  partition_key=PartitionKey(path='/id'),\n",
    "                                                  indexing_policy=indexing_policy, \n",
    "                                                  vector_embedding_policy=vector_embedding_policy,\n",
    "                                                  offer_throughput=10000) \n",
    "    print('Container with id \\'{0}\\' created'.format(movies_container.id)) \n",
    "\n",
    "except exceptions.CosmosHttpResponseError: \n",
    "    raise \n",
    "\n",
    "# Create the cache collection with vector index\n",
    "try:\n",
    "    cache_container = db.create_container_if_not_exists(id=cosmos_cache, \n",
    "                                                  partition_key=PartitionKey(path='/id'), \n",
    "                                                  indexing_policy=indexing_policy,\n",
    "                                                  vector_embedding_policy=vector_embedding_policy,\n",
    "                                                  offer_throughput=1000) \n",
    "    print('Container with id \\'{0}\\' created'.format(cache_container.id)) \n",
    "\n",
    "except exceptions.CosmosHttpResponseError: \n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tenacity import retry, stop_after_attempt, wait_random_exponential \n",
    "import logging\n",
    "@retry(wait=wait_random_exponential(min=2, max=300), stop=stop_after_attempt(20))\n",
    "def generate_embeddings(text):\n",
    "    try:        \n",
    "        response = openai_client.embeddings.create(\n",
    "            input=text,\n",
    "            model=openai_embeddings_deployment,\n",
    "            dimensions=openai_embeddings_dimensions\n",
    "        )\n",
    "        embeddings = response.model_dump()\n",
    "        return embeddings['data'][0]['embedding']\n",
    "    except Exception as e:\n",
    "        # Log the exception with traceback for easier debugging\n",
    "        logging.error(\"An error occurred while generating embeddings.\", exc_info=True)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the data file\n",
    "with zipfile.ZipFile(\"../../DataSet/Movies/MovieLens-4489-256D.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"/Data\")\n",
    "zip_ref.close()\n",
    "\n",
    "# Load the data file\n",
    "data = []\n",
    "with open('/Data/MovieLens-4489-256D.json', 'r') as d:\n",
    "    data = json.load(d)\n",
    "\n",
    "# View the number of documents in the data (4489)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following code to get raw movies data is commented out in favour of\n",
    "#getting prevectorized data. If you want to vectorize the raw data from\n",
    "#storage_file_url, uncomment the below, and set vectorizeFlag=True\n",
    "\n",
    "#data = urllib.request.urlopen(storage_file_url)\n",
    "#data = json.load(data)\n",
    "\n",
    "vectorizeFlag=False\n",
    "\n",
    "import asyncio\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "async def generate_vectors(items, vector_property):\n",
    "    # Create a thread pool executor for the synchronous generate_embeddings\n",
    "    loop = asyncio.get_event_loop()\n",
    "    \n",
    "    # Define a function to call generate_embeddings using run_in_executor\n",
    "    async def generate_embedding_for_item(item):\n",
    "        try:\n",
    "            # Offload the sync generate_embeddings to a thread\n",
    "            vectorArray = await loop.run_in_executor(None, generate_embeddings, item['overview'])\n",
    "            item[vector_property] = vectorArray\n",
    "        except Exception as e:\n",
    "            # Log or handle exceptions if needed\n",
    "            logging.error(f\"Error generating embedding for item: {item['overview'][:50]}...\", exc_info=True)\n",
    "    \n",
    "    # Create tasks for all the items to generate embeddings concurrently\n",
    "    tasks = [generate_embedding_for_item(item) for item in items]\n",
    "    \n",
    "    # Run all the tasks concurrently and wait for their completion\n",
    "    await asyncio.gather(*tasks)\n",
    "    \n",
    "    return items\n",
    "\n",
    "async def insert_data(vectorize=False):\n",
    "    start_time = time.time()  # Record the start time\n",
    "    \n",
    "    # If vectorize flag is True, generate vectors for the data\n",
    "    if vectorize:\n",
    "        print(\"Vectorizing data, please wait...\")\n",
    "        global data\n",
    "        data = await generate_vectors(data, \"vector\")\n",
    "\n",
    "    counter = 0\n",
    "    tasks = []\n",
    "    max_concurrency = 5  # Adjust this value to control the level of concurrency\n",
    "    semaphore = asyncio.Semaphore(max_concurrency)\n",
    "    print(\"Starting doc load, please wait...\")\n",
    "    \n",
    "    def upsert_item_sync(obj):\n",
    "        movies_container.upsert_item(body=obj)\n",
    "    \n",
    "    async def upsert_object(obj):\n",
    "        nonlocal counter\n",
    "        async with semaphore:\n",
    "            await asyncio.get_event_loop().run_in_executor(None, upsert_item_sync, obj)\n",
    "            # Progress reporting\n",
    "            counter += 1\n",
    "            if counter % 100 == 0:\n",
    "                print(f\"Sent {counter} documents for insertion into collection.\")\n",
    "    \n",
    "    for obj in data:\n",
    "        tasks.append(asyncio.create_task(upsert_object(obj)))\n",
    "    \n",
    "    # Run all upsert tasks concurrently within the limits set by the semaphore\n",
    "    await asyncio.gather(*tasks)\n",
    "    \n",
    "    end_time = time.time()  # Record the end time\n",
    "    duration = end_time - start_time  # Calculate the duration\n",
    "    print(f\"All {counter} documents inserted!\")\n",
    "    print(f\"Time taken: {duration:.2f} seconds ({duration:.3f} milliseconds)\")\n",
    "\n",
    "# Run the async function with the vectorize flag set to True or False as needed\n",
    "await insert_data(vectorizeFlag)  # or await insert_data() for default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(container, vectors, similarity_score=0.02, num_results=5):\n",
    "    results = container.query_items(\n",
    "        query='''\n",
    "        SELECT TOP @num_results c.overview, VectorDistance(c.vector, @embedding) as SimilarityScore \n",
    "        FROM c\n",
    "        WHERE VectorDistance(c.vector,@embedding) > @similarity_score\n",
    "        ORDER BY VectorDistance(c.vector,@embedding)\n",
    "        ''',\n",
    "        parameters=[\n",
    "            {\"name\": \"@embedding\", \"value\": vectors},\n",
    "            {\"name\": \"@num_results\", \"value\": num_results},\n",
    "            {\"name\": \"@similarity_score\", \"value\": similarity_score}\n",
    "        ],\n",
    "        enable_cross_partition_query=True,\n",
    "        populate_query_metrics=True\n",
    "    )\n",
    "    results = list(results)\n",
    "    formatted_results = [{'SimilarityScore': result.pop('SimilarityScore'), 'document': result} for result in results]\n",
    "\n",
    "    return formatted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat_history(container, completions=3):\n",
    "    results = container.query_items(\n",
    "        query='''\n",
    "        SELECT TOP @completions *\n",
    "        FROM c\n",
    "        ORDER BY c._ts DESC\n",
    "        ''',\n",
    "        parameters=[\n",
    "            {\"name\": \"@completions\", \"value\": completions},\n",
    "        ],\n",
    "        enable_cross_partition_query=True\n",
    "    )\n",
    "    results = list(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_completion(user_prompt, vector_search_results, chat_history):\n",
    "    system_prompt = '''\n",
    "    You are an intelligent assistant for movies. You are designed to provide helpful answers to user questions about movies in your database.\n",
    "    You are friendly, helpful, and informative and can be lighthearted. Be concise in your responses, but still friendly.\n",
    "     - Only answer questions related to the information provided below. Provide at least 3 candidate movie answers in a list.\n",
    "     - Write two lines of whitespace between each answer in the list.\n",
    "    '''\n",
    "\n",
    "    messages = [{'role': 'system', 'content': system_prompt}]\n",
    "    for chat in chat_history:\n",
    "        messages.append({'role': 'user', 'content': chat['prompt'] + \" \" + chat['completion']})\n",
    "    messages.append({'role': 'user', 'content': user_prompt})\n",
    "    for result in vector_search_results:\n",
    "        messages.append({'role': 'system', 'content': json.dumps(result['document'])})\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=openai_completions_deployment,\n",
    "        messages=messages,\n",
    "        temperature=0.1\n",
    "    )    \n",
    "    return response.model_dump()\n",
    "\n",
    "def chat_completion(cache_container, movies_container, user_input):\n",
    "    print(\"starting completion\")\n",
    "    # Generate embeddings from the user input\n",
    "    user_embeddings = generate_embeddings(user_input)\n",
    "    # Query the chat history cache first to see if this question has been asked before\n",
    "    cache_results = get_cache(container=cache_container, vectors=user_embeddings, similarity_score=0.99, num_results=1)\n",
    "    if len(cache_results) > 0:\n",
    "        print(\"Cached Result\\n\")\n",
    "        return cache_results[0]['completion'], True\n",
    "        \n",
    "    else:\n",
    "        # Perform vector search on the movie collection\n",
    "        print(\"New result\\n\")\n",
    "        search_results = vector_search(movies_container, user_embeddings)\n",
    "\n",
    "        print(\"Getting Chat History\\n\")\n",
    "        # Chat history\n",
    "        chat_history = get_chat_history(cache_container, 3)\n",
    "        # Generate the completion\n",
    "        print(\"Generating completions \\n\")\n",
    "        completions_results = generate_completion(user_input, search_results, chat_history)\n",
    "\n",
    "        print(\"Caching response \\n\")\n",
    "        # Cache the response\n",
    "        cache_response(cache_container, user_input, user_embeddings, completions_results)\n",
    "\n",
    "        print(\"\\n\")\n",
    "        # Return the generated LLM completion\n",
    "        return completions_results['choices'][0]['message']['content'], False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_response(container, user_prompt, prompt_vectors, response):\n",
    "    chat_document = {\n",
    "        'id': str(uuid.uuid4()),\n",
    "        'prompt': user_prompt,\n",
    "        'completion': response['choices'][0]['message']['content'],\n",
    "        'completionTokens': str(response['usage']['completion_tokens']),\n",
    "        'promptTokens': str(response['usage']['prompt_tokens']),\n",
    "        'totalTokens': str(response['usage']['total_tokens']),\n",
    "        'model': response['model'],\n",
    "        'vector': prompt_vectors\n",
    "    }\n",
    "    container.create_item(body=chat_document)\n",
    "\n",
    "def get_cache(container, vectors, similarity_score=0.0, num_results=5):\n",
    "    # Execute the query\n",
    "    results = container.query_items(\n",
    "        query= '''\n",
    "        SELECT TOP @num_results *\n",
    "        FROM c\n",
    "        WHERE VectorDistance(c.vector,@embedding) > @similarity_score\n",
    "        ORDER BY VectorDistance(c.vector,@embedding)\n",
    "        ''',\n",
    "        parameters=[\n",
    "            {\"name\": \"@embedding\", \"value\": vectors},\n",
    "            {\"name\": \"@num_results\", \"value\": num_results},\n",
    "            {\"name\": \"@similarity_score\", \"value\": similarity_score},\n",
    "        ],\n",
    "        enable_cross_partition_query=True, populate_query_metrics=True)\n",
    "    results = list(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Cosmic Movie Assistant\")\n",
    "    msg = gr.Textbox(label=\"Ask me about movies in the Cosmic Movie Database!\")\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def user(user_message, chat_history):\n",
    "        start_time = time.time()\n",
    "        response_payload, cached = chat_completion(cache_container, movies_container, user_message)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = round((end\n",
    "        time - start_time) * 1000, 2)\n",
    "        details = f\"\\n (Time: {elapsed_time}ms)\"\n",
    "        if cached:\n",
    "            details += \" (Cached)\"\n",
    "        chat_history.append([user_message, response_payload + details])\n",
    "        \n",
    "        return gr.update(value=\"\"), chat_history\n",
    "    \n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False)\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "# Launch the Gradio interface\n",
    "demo.launch(debug=True)\n",
    "\n",
    "# Be sure to run this cell to close or restart the Gradio demo\n",
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
